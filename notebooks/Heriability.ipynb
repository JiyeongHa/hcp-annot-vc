{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05de482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import neuropythy as ny\n",
    "import matplotlib as mpl\n",
    "import itertools\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from visualization import plot_annot_properties as vis\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2921ad1f",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06685882",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path       = Path('/data/crcns2021/hcpannot-cache')\n",
    "image_cache_path = cache_path / 'annot-images'\n",
    "v123_cache_path  = cache_path / 'annot-v123'\n",
    "csulc_cache_path = cache_path / 'annot-csulc'\n",
    "# The save path of the data branch:\n",
    "data_path = Path('/data/crcns2021/results/data_branch/save')\n",
    "# The processing path and subpaths:\n",
    "proc_path    = Path('/data/crcns2021/results/proc')\n",
    "traces_path  = proc_path / 'traces'\n",
    "paths_path   = proc_path / 'paths'\n",
    "means_path   = proc_path / 'means'\n",
    "labels_path  = proc_path / 'labels'\n",
    "reports_path = proc_path / 'reports'\n",
    "fig_dir      = proc_path / 'figures'\n",
    "\n",
    "# The file of visual surface areas for the ventral data.\n",
    "ventral_sarea_path = proc_path / 'ventral_sareas.tsv'\n",
    "\n",
    "# The hcpannot library path; if hcpannot is not on the path for\n",
    "# this notebook, the notebook will try to figure out where it is\n",
    "# and will use this directory as a backup.\n",
    "hcpannot_lib_path = Path('~/code/hcp-annot-vc_analysis')\n",
    "\n",
    "# If you aren't using /data\n",
    "os.environ['HCPANNOT_LOAD_PATH'] = os.fspath(cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14fd61",
   "metadata": {},
   "source": [
    "## Font settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d332b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional matplotlib preferences:\n",
    "font_data = {'family':'sans-serif',\n",
    "             'sans-serif':['HelveticaNeue', 'Helvetica', 'Arial'],\n",
    "             'size': 10,\n",
    "             'weight': 'light'}\n",
    "mpl.rc('font',**font_data)\n",
    "# we want relatively high-res images, especially when saving to disk.\n",
    "mpl.rcParams['figure.dpi'] = 72*2\n",
    "mpl.rcParams['savefig.dpi'] = 72*4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e01db",
   "metadata": {},
   "source": [
    "## hcpannot settings and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b588de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we can find the hcpannot library. If we were given a path for the\n",
    "# hcpannot library, we add it to the system path here.\n",
    "if hcpannot_lib_path:\n",
    "    hlp = os.path.expandvars(os.fspath(hcpannot_lib_path))\n",
    "    hlp = os.path.expanduser(hlp)\n",
    "    if hlp.strip('/').endswith('hcpannot'):\n",
    "        warnings.warn(\n",
    "            \"adding hcpannot_lib_path that ends with 'hcpannot';\"\n",
    "            \" the hcpannot_lib_path should contain the hcpannot directory\")\n",
    "    if os.path.isdir(hlp):\n",
    "        sys.path.append(hlp)\n",
    "    else:\n",
    "        raise ValueError(\"hcpannot_lib_path is not a directory\")\n",
    "# We try importing it and if we fail, we check to see if we are just running\n",
    "# the notebook from the work directory and try again.\n",
    "try:\n",
    "    import hcpannot\n",
    "except ModuleNotFoundError:\n",
    "    hcpannot = None\n",
    "    if os.getcwd().endswith('/work'):\n",
    "        hlp = os.path.abspath('..')\n",
    "        sys.path.append(hlp)\n",
    "        \n",
    "# Now we can import the hcp-annot-vc library. (It may have been imported\n",
    "# above in the try block, but that's fine!)\n",
    "import hcpannot\n",
    "\n",
    "# Note the cache path we want to use outside the docker container;\n",
    "# normally this gets set by the Docker startup, so here outside\n",
    "# the docker container we set it manually.\n",
    "hcpannot.interface.default_load_path = str(cache_path)\n",
    "# We have to update some data after setting this.\n",
    "\n",
    "# Subject lists. These are defined in the analysis subpackage of\n",
    "# the hcpannot library, and subject_list_<x> is the <x>th list of\n",
    "# subject IDs that we assigned.\n",
    "from hcpannot.config import (\n",
    "    subject_list,\n",
    "    subject_list_1,\n",
    "    subject_list_2,\n",
    "    subject_list_3)\n",
    "# The subject IDs we are processing over, as a numpy array.\n",
    "sids = np.array(subject_list)\n",
    "\n",
    "# The mean rater's name ('mean') and some plotting functions are also defined in\n",
    "# the hcpannot.analysis subpackage.\n",
    "from hcpannot.analysis import plot_contours\n",
    "\n",
    "# Finally, the proc and meanproc functions, which give us the processed data.\n",
    "from hcpannot.proc import proc, meanproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f0b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_report(region, rater, sid, h):\n",
    "    \"\"\"Loads surface area report for the region, rater, sid, and hemisphere.\n",
    "    \n",
    "    The region should be the name of one of the contour regions, e.g.\n",
    "    `'ventral'`. Loads a dictionary of the processing report for the given\n",
    "    rater, subject, and hemisphere, and returns a processed version of that\n",
    "    report. The processing includes both square-mm and percentage reports of the\n",
    "    surface area.\n",
    "    \n",
    "    If the file for the report is not found, it is skipped and the values are\n",
    "    left as NaN.\n",
    "    \"\"\"\n",
    "    from json import load\n",
    "    data = {\n",
    "        'rater':rater,\n",
    "        'sid':sid,\n",
    "        'hemisphere':h}\n",
    "    for k in region_areas[region]:\n",
    "        data[f'{k}_mm2'] = np.nan\n",
    "        data[f'{k}_percent'] = np.nan\n",
    "    try:\n",
    "        path = os.path.join(reports_path, rater, str(sid))\n",
    "        flnm = os.path.join(path, f'{h}.{region}_sareas.json')\n",
    "        with open(flnm, 'rt') as fl:\n",
    "            sarea = load(fl)\n",
    "        for (k,v) in sarea.items():\n",
    "            data[f'{k}_mm2'] = v\n",
    "            if k != 'cortex':\n",
    "                data[f'{k}_percent'] = v * 100 / sarea['cortex']\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return data\n",
    "def load_allreports(region, include_mean=True, sids=subject_list):\n",
    "    \"\"\"Loads all reports for a region and returns a dataframe of them.\n",
    "    \n",
    "    This runs `load_report` over all raters, subjects, and hemispheres and\n",
    "    returns a dataframe of all the reports. If a report file is not found,\n",
    "    then the row is left with NaNs indicating missing data.\n",
    "    \"\"\"\n",
    "    if include_mean:\n",
    "        if include_mean == True:\n",
    "            include_mean = 'mean'\n",
    "        include_mean = [include_mean]\n",
    "    else:\n",
    "        include_mean = []\n",
    "    raters = (region_raters[region] + include_mean)\n",
    "    return pd.DataFrame(\n",
    "        [load_report(region, rater, sid, h)\n",
    "         for rater in raters\n",
    "         for sid in sids\n",
    "         for h in ('lh', 'rh')])\n",
    "def nestget(d, k):\n",
    "    \"\"\"Retrieves nested data from the proc dictionaries.\n",
    "    \n",
    "    Certain keys such as `'boundaries'` are accessible in the dictionaries that\n",
    "    are returned by the `proc` function only via the `'nested_data'` key, which\n",
    "    typically contains another proc dictionary with additional data. The\n",
    "    `nestget` function gets data from these embedded dictionaries.\n",
    "    \"\"\"\n",
    "    while k not in d:\n",
    "        d = d['nested_data']\n",
    "    return d[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cec380",
   "metadata": {},
   "source": [
    "## Annotation meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e9cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sets of raters we use.\n",
    "ventral_raters = [\n",
    "    'BrendaQiu',\n",
    "    'bogengsong',\n",
    "    'JiyeongHa',\n",
    "    'lindazelinzhao',\n",
    "    'nourahboujaber',\n",
    "    'jennifertepan']\n",
    "region_raters = {\n",
    "    'ventral': ventral_raters}\n",
    "\n",
    "# The visual areas included in each pipeline/region.\n",
    "region_areas = {\n",
    "    'early': ('V1', 'V2', 'V3'),\n",
    "    'ventral': ('hV4', 'VO1', 'VO2', 'VO'),\n",
    "    'dorsal': ('V3a', 'V3b', 'IPS0', 'LO1')}\n",
    "\n",
    "roi_list = []\n",
    "roi_list += [k for k in region_areas['early']]\n",
    "roi_list += [k for k in region_areas['ventral']]\n",
    "roi_list += ['cortex']\n",
    "\n",
    "# The hemispheres we are processing over.\n",
    "hemis = ['lh', 'rh']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeea2f1",
   "metadata": {},
   "source": [
    "# Load early and ventral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_early_df = ny.data['hcp_lines'].surface_area_dataframe\n",
    "cols = ['sid','anatomist','roiLV1','roiLV2','roiLV3','roiRV1','roiRV2','roiRV3']\n",
    "new_cols = ['sid','researcher','lh_V1_mm2','lh_V2_mm2','lh_V3_mm2','rh_V1_mm2','rh_V2_mm2','rh_V3_mm2']\n",
    "wide_early_df = wide_early_df[cols]\n",
    "wide_early_df = wide_early_df.rename(columns=dict(zip(cols, new_cols)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vwm",
   "language": "python",
   "name": "vwm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
