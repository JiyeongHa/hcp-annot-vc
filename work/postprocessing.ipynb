{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cache directories:\n",
    "cache_path       = '/data/crcns2021'\n",
    "image_cache_path = f'{cache_path}/annot-images'\n",
    "v123_cache_path  = f'{cache_path}/annot-v123'\n",
    "csulc_cache_path = f'{cache_path}/annot-csulc'\n",
    "trace_save_path = '/home/nben/code/hcp-annot-vc_data/save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pimms, pandas, warnings, urllib, datetime\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import nibabel as nib\n",
    "import neuropythy as ny\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyvolume as ipv\n",
    "import torch, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional matplotlib preferences:\n",
    "font_data = {'family':'sans-serif',\n",
    "             'sans-serif':['HelveticaNeue', 'Helvetica', 'Arial'],\n",
    "             'size': 10,\n",
    "             'weight': 'light'}\n",
    "mpl.rc('font',**font_data)\n",
    "# we want relatively high-res images, especially when saving to disk.\n",
    "mpl.rcParams['figure.dpi'] = 72*2\n",
    "mpl.rcParams['savefig.dpi'] = 72*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import `hcpannot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we are in the right directory for this!\n",
    "cwd = os.getcwd()\n",
    "if cwd.endswith('/work'):\n",
    "    os.chdir('..')\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "# Now we can import the hcp-annot-v123 library.\n",
    "import hcpannot\n",
    "\n",
    "# Note the cache path we want to use outside the docker container;\n",
    "# normally this gets set by the Docker startup, so here outside\n",
    "# the docker container we set it manually.\n",
    "hcpannot.interface.default_load_path = cache_path\n",
    "# We have to update some data after setting this.\n",
    "\n",
    "# We also want to grab a few variables from hcpannot:\n",
    "from hcpannot.interface import default_imshape as roi_image_shape\n",
    "roi_image_shape = roi_image_shape[0] // 2\n",
    "from hcpannot.interface import imgrid_to_flatmap\n",
    "from hcpannot.interface import flatmap_to_imgrid\n",
    "# The analysis plan (for processing contours):\n",
    "from hcpannot import vc_plan\n",
    "# The functions for loading and saving contours.\n",
    "from hcpannot import (save_contours, load_contours)\n",
    "\n",
    "# Subject lists. These are defined in the analysis subpackage of\n",
    "# the hcpannot library, and subject_list_<x> is the <x>th list of\n",
    "# subject IDs that we assigned.\n",
    "from hcpannot import (subject_ids, subject_list_1,\n",
    "                      subject_list_2, subject_list_3)\n",
    "sids = np.array(hcpannot.subject_ids)\n",
    "\n",
    "# We also want to import the IO functions.\n",
    "from hcpannot.io import (save_traces, export_traces, load_traces, export_paths,\n",
    "                         load_paths, export_means, calc_surface_areas)\n",
    "\n",
    "# The mean rater's name ('mean') is also defined in hcpannot.\n",
    "from hcpannot.analysis import meanrater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_colors = {\n",
    "    'hV4_outer': (0.5, 0,   0),\n",
    "    'hV4_VO1':   (0,   0.3, 0),\n",
    "    'VO_outer':  (0,   0.4, 0.6),\n",
    "    'VO1_VO2':   (0,   0,   0.5)}\n",
    "preproc_colors = {\n",
    "    'hV4_outer': (0.7, 0,   0),\n",
    "    'hV4_VO1':   (0,   0.5, 0),\n",
    "    'VO_outer':  (0,   0.6, 0.8),\n",
    "    'VO1_VO2':   (0,   0,   0.7),\n",
    "    'V3_ventral':(0.7, 0,   0.7),\n",
    "    'outer':     (0.7, 0.7, 0, 0)}\n",
    "ext_colors = {\n",
    "    'hV4_outer': (1,   0,   0),\n",
    "    'hV4_VO1':   (0,   0.8, 0),\n",
    "    'VO_outer':  (0,   0.9, 1),\n",
    "    'VO1_VO2':   (0,   0,   1),\n",
    "    'V3_ventral':(0.8, 0,   0.8),\n",
    "    'outer':     (0.8, 0.8, 0.8, 1)}\n",
    "boundary_colors = {\n",
    "    'hV4': (1, 0.5, 0.5),\n",
    "    'VO1': (0.5, 1, 0.5),\n",
    "    'VO2': (0.5, 0.5, 1)}\n",
    "\n",
    "def plot_contours(dat, raw=None, ext=None, preproc=None,\n",
    "                  contours=None, boundaries=None,\n",
    "                  figsize=(2,2), dpi=(72*5), axes=None, \n",
    "                  flatmap=True, lw=1, color='prf_polar_angle',  \n",
    "                  mask=('prf_variance_explained', 0.05, 1)):\n",
    "    # Make the figure.\n",
    "    if axes is None:\n",
    "        (fig,ax) = plt.subplots(1,1, figsize=figsize, dpi=dpi)\n",
    "        fig.subplots_adjust(0,0,1,1,0,0)\n",
    "    else:\n",
    "        ax = axes\n",
    "        fig = ax.get_figure()\n",
    "    # Plot the flatmap.\n",
    "    if flatmap:\n",
    "        fmap = dat['flatmap']\n",
    "        ny.cortex_plot(fmap, color=color, mask=mask, axes=ax)\n",
    "    # Plot the requested lines:\n",
    "    if raw is not None:\n",
    "        if raw is True: raw = raw_colors\n",
    "        for (k,v) in dat['raw_contours'].items():\n",
    "            c = raw.get(k, 'w')\n",
    "            ax.plot(v[0], v[1], '-', color=c, lw=lw)\n",
    "    if preproc is not None:\n",
    "        if preproc is True: preproc = preproc_colors\n",
    "        for (k,v) in dat['preproc_contours'].items():\n",
    "            c = preproc.get(k, 'w')\n",
    "            ax.plot(v[0], v[1], '-', color=c, lw=lw)\n",
    "    if ext is not None:\n",
    "        if ext is True: ext = ext_colors\n",
    "        for (k,v) in dat['ext_contours'].items():\n",
    "            c = ext.get(k, 'w')\n",
    "            ax.plot(v[0], v[1], '-', color=c, lw=lw)\n",
    "    if contours is not None:\n",
    "        if contours is True: contours = ext_colors\n",
    "        for (k,v) in dat['contours'].items():\n",
    "            c = contours.get(k, 'w')\n",
    "            ax.plot(v[0], v[1], '-', color=c, lw=lw)\n",
    "    if boundaries is not None:\n",
    "        if boundaries is True: boundaries = boundary_colors\n",
    "        for (k,v) in dat['boundaries'].items():\n",
    "            c = boundaries.get(k, 'w')\n",
    "            x = np.concatenate([v[0], [v[0][0]]])\n",
    "            y = np.concatenate([v[1], [v[1][0]]])\n",
    "            ax.plot(x, y, '-', color=c, lw=lw)\n",
    "    ax.axis('off')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example code-block using the above to plot a set of boundaries along with the V1-V3 contours.\n",
    "\n",
    "```python\n",
    "dat = vc_plan(rater='BrendaQiu',\n",
    "              sid=111312,\n",
    "              hemisphere='lh',\n",
    "              save_path=trace_save_path)\n",
    "# Make the flatmap plot and the boundaries.\n",
    "fig = plot_contours(dat, boundaries=True)\n",
    "# Extract the pyplot axes that were used.\n",
    "ax = fig.axes[0]\n",
    "# Grab the subject data, which includes the V1-V3 contours.\n",
    "sdat = hcpannot.interface.subject_data[(dat['sid'],dat['hemisphere'])]\n",
    "# And plot all of these contours:\n",
    "for (x,y) in sdat['v123'].values():\n",
    "    ax.plot(x, y, 'w-', lw=0.25)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export/Multiprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets used below to pass tuples of arguments to functions through\n",
    "# the multiprocessing pool interface.\n",
    "def tupcall(fn, tup=None,\n",
    "            onfail=None,\n",
    "            onokay=None,\n",
    "            retryon=None):\n",
    "    \"\"\"Passes a tuple of arguments to a function.\n",
    "\n",
    "    `tupcall(fn, tuple)` calls `fn(*tuple)` and returns the result. If the final\n",
    "    element of `tuple` is a dictionary, then the return value is instead\n",
    "    `fn(*tuple[:-1], **tuple[-1])`. An empty dictionary may be passed as the\n",
    "    final element of `tuple`.\n",
    "    \n",
    "    `tupcall(fn)` returns a function `g` such that `g(tuple)` is equivalent to\n",
    "    `tupcall(fn, tuple)`.\n",
    "    \n",
    "    The optional argument `onfail` can be set to a backup function that is \n",
    "    called if an exception is raised. In such a case, the return value of the\n",
    "    `tupcall` function is `onfail(raised_exception, tuple)`.\n",
    "    \n",
    "    The optional argument `onokay` can be set to a function that is \n",
    "    called if no exception is raised. In such a case, the return value of the\n",
    "    `tupcall` function is `onokay(raised_error, tuple)`.\n",
    "\n",
    "    The optional argument `retryon` can be set to an exception type or a tuple\n",
    "    of exception types; if one of these exceptions is raised during execution,\n",
    "    then the function is retried once. Alternately, `retryon` may be a dict\n",
    "    whose keys are exception types or tuples of exception types and whose values\n",
    "    are functions that determine whether or not to retry the call. These\n",
    "    functions are called as `do_retry = fn(raised_error, tuple)`, and `do_retry`\n",
    "    must be a boolean result.\n",
    "    \"\"\"\n",
    "    if tup is None:\n",
    "        from functools import partial\n",
    "        return partial(tupcall, fn,\n",
    "                       onfail=onfail, onokay=onokay, retryon=retryon)\n",
    "    elif not isinstance(tup, (tuple, list)):\n",
    "        raise ValueError(\"tupcall requires a tuple or list\")\n",
    "    elif len(tup) == 0:\n",
    "        fin = {}\n",
    "        arg = tup\n",
    "    else:\n",
    "        fin = tup[-1]\n",
    "        if isinstance(fin, dict):\n",
    "            arg = tup[:-1]\n",
    "        else:\n",
    "            fin = {}\n",
    "            arg = tup\n",
    "    if retryon is None and onfail is None:\n",
    "        res = fn(*arg, **fin)\n",
    "    else:\n",
    "        while True:\n",
    "            try:\n",
    "                res = fn(*arg, **fin)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if retryon is not None:\n",
    "                    if isinstance(retryon, dict):\n",
    "                        do_retry = False\n",
    "                        for (k,fn) in retryon.items():\n",
    "                            if isinstance(e, k):\n",
    "                                do_retry = fn(e, tup)\n",
    "                                if do_retry: break\n",
    "                        if do_retry: continue\n",
    "                    elif isinstance(e, retryon):\n",
    "                        # We only retry once.\n",
    "                        retryon = None\n",
    "                        continue\n",
    "                # If we reach here, we aren't retrying.\n",
    "                if onfail:\n",
    "                    return onfail(e, tup)\n",
    "                else:\n",
    "                    raise                    \n",
    "    if onokay is not None:\n",
    "        return onokay(res, tup)\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okaystr(res, tup):\n",
    "    \"\"\"Returns a string summary of an okay result for use with `tupcall`.\n",
    "    \n",
    "    `okaystr(r, tuple)` returns a string `f\"OKAY: {tuple}\"`.\n",
    "    \"\"\"\n",
    "    return f\"OKAY: {tup}\"\n",
    "def failstr(err, tup):\n",
    "    \"\"\"Returns a string summary of a failed result for use with `tupcall`.\n",
    "    \n",
    "    `failstr(err, tuple)` returns a string `f\"FAIL: {tuple} {err}\"`.\n",
    "    \"\"\"\n",
    "    return f\"FAIL: {tup} {err}\"\n",
    "def retry_sleep(err=None, tup=None, duration=Ellipsis):\n",
    "    \"\"\"Sleeps for 5 seconds then returns `True`, for use with `tupcall`.\n",
    "\n",
    "    `retry_sleep(error, tuple)` sleeps for 5 seconds then returns `True`.\n",
    "    \n",
    "    `retry_sleep(error, tuple, duration=dur)` sleeps for `dur` seconds then\n",
    "    returns `True`.\n",
    "    \n",
    "    `retry_sleep(dur)` returns a function equivalent to\n",
    "    `lambda err, tup: retry_sleep(err, tup, duration=dur)`.\n",
    "    \n",
    "    This function is intended for use with `tupcall`'s `retryon` option, for\n",
    "    example, `tupcall(fn, tup, retryon={HTTPError: retry_sleep(5)})`.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    if err is None and tup is None:\n",
    "        dur = duration\n",
    "    elif tup is None and duration is Ellipsis:\n",
    "        dur = err\n",
    "    elif tup is None:\n",
    "        raise ValueError(f\"invalid arguments to retry_sleep:\"\n",
    "                         f\" ({err},{tup},{duration})\")\n",
    "    else:\n",
    "        dur = 5 if duration is Ellipsis else duration\n",
    "        time.sleep(dur)\n",
    "        return True\n",
    "    # If we make it here, we need to return a partial function.\n",
    "    return lambda err,tup: retry_sleep(err, tup, duration=dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We actually want to retry on HTTPError, which occurs when we overload OSF with\n",
    "# download requests.\n",
    "def mprun(fn, jobs,\n",
    "          nproc=None,\n",
    "          print=print,\n",
    "          onokay=okaystr,\n",
    "          onfail=failstr,\n",
    "          retryon={urllib.error.HTTPError: retry_sleep}):\n",
    "    \"\"\"Runs a function across many processes via the `multiprocessing` package.\n",
    "    \n",
    "    `mprun(fn, joblist)` runs the given function across as many processes as\n",
    "    there are CPUs for each job in `joblist`. The jobs should be tuples that\n",
    "    can be executed via `tupcall(fn, job)`.\n",
    "    \n",
    "    The optional arguments `onfail` and `onokay` are passed through to the\n",
    "    `tupcall` function. Additionally, the option `nproc` can be set to the\n",
    "    number of processes that should be used; the default of `None` indicates\n",
    "    that the number of processes should match the number of CPUs. Finally, the\n",
    "    option `print` may be set to a print function that is used to log the\n",
    "    progress of the run. If `None` is given, then no printing is done;\n",
    "    otherwise, every 10% of the total set of jobs complete produces a progress\n",
    "    message.\n",
    "    \"\"\"\n",
    "    import multiprocessing as mp\n",
    "    # Process the arguments.\n",
    "    njobs = len(jobs)\n",
    "    try:\n",
    "        fnname = fn.__name__\n",
    "    except Exception:\n",
    "        fnname = str(fn)\n",
    "    if nproc is None:\n",
    "        nproc = mp.cpu_count()\n",
    "    callfn = tupcall(fn, onfail=onfail, onokay=onokay, retryon=retryon)\n",
    "    # Start the jobs!\n",
    "    print(f\"Beginning {njobs} jobs with tag '{tag}'...\")\n",
    "    donecount = 0\n",
    "    res = []\n",
    "    for ii in range(0, njobs, nproc):\n",
    "        jj = min(ii + nproc, njobs)\n",
    "        nn = jj - ii\n",
    "        with mp.Pool(nn) as pool:\n",
    "            r = pool.map(callfn, jobs[ii:jj])\n",
    "        for rr in r:\n",
    "            res.append(rr)\n",
    "        if donecount * 10 // njobs < (donecount + nn) * 10 // njobs:\n",
    "            print(\" - %4d / %4d (%3d%%)\" % (jj, njobs, int(100*jj/njobs)))\n",
    "        donecount += nn\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpstep(fn, jobs, tag,\n",
    "           overwrite=False, nproc=None, save_path=trace_save_path,\n",
    "           onokay=okaystr, onfail=failstr, print=print,\n",
    "           retryon={urllib.error.HTTPError: retry_sleep}):\n",
    "    \"\"\"Runs one multiprocessing step in the contour process/export workflow.\n",
    "    \n",
    "    `mpstep(fn, jobs, tag)` is designed to be run with the `hcpannot.io`\n",
    "    functions for exporting processed data about the contours:\n",
    "      * `export_traces`\n",
    "      * `export_paths`\n",
    "      * `export_means`\n",
    "    Each of these functions must be multiprocessed across many combinations of\n",
    "    raters, subjects, and hemispheres. These arguments must be listed in `jobs`.\n",
    "    The `tag` is used to name the logfile that is exported on success.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fn : function\n",
    "        The function that is to be multiprocessed across all jobs.\n",
    "    jobs : list of tuples\n",
    "        A list of arguments to `fn`; see `makejobs` and `tupcall`.\n",
    "    tag : str\n",
    "        A tag name used to identify the logfile, which is placed in the\n",
    "        `save_path` directory.\n",
    "    overwrite : boolean, optional\n",
    "        If overwrite is `False` and the logfile already exists, then it is read\n",
    "        in and returned instead of rerunning the jobs. The default is `False`.\n",
    "    nproc : int or None, optional\n",
    "        The number of processes to multiplex across. If this is `None`, then the\n",
    "        number of CPUs is used. The default is `None`.\n",
    "    save_path : directory name, optional\n",
    "        The directory from which to load the contour data and into which to\n",
    "        write the logfile. The default is the global variable `trace_save_path`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        A list with one entry per job that is the `okaystr` or `failstr`\n",
    "        representation of that job's return value.\n",
    "    \"\"\"\n",
    "    # We'll write out this logfile.\n",
    "    #logfile = f\"{tag}_{datetime.datetime.now().isoformat()}.log\"\n",
    "    logfile = f\"proc_{tag}.log\"\n",
    "    logfile = os.path.join(save_path, logfile)\n",
    "    # Multiprocess all the jobs.\n",
    "    if overwrite or not os.path.isfile(logfile):\n",
    "        proc_results = mprun(fn, jobs,\n",
    "                             nproc=nproc,\n",
    "                             print=print,\n",
    "                             onokay=onokay,\n",
    "                             onfail=onfail,\n",
    "                             retryon=retryon)\n",
    "        # Write out a log of these results.\n",
    "        with open(logfile, \"wt\") as fl:\n",
    "            fl.write('\\n'.join(proc_results))\n",
    "            fl.write('\\n')\n",
    "    else:\n",
    "        with open(logfile, \"rt\") as fl:\n",
    "            proc_results = fl.readlines()\n",
    "    return proc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makejobs(*args):\n",
    "    \"Equivalent to `itertools.product(*args)` but always returns a list.\"\n",
    "    from itertools import product\n",
    "    return list(product(*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Contours into Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These parameters are used throughout the code below that process the contours into area boundaries and surface areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raters we are processing over.\n",
    "proc_raters = [\n",
    "    'BrendaQiu',\n",
    "    'bogengsong',\n",
    "    'JiyeongHa',\n",
    "    'lindazelinzhao',\n",
    "    'nourahboujaber',\n",
    "    'jennifertepan']\n",
    "\n",
    "# The subject IDs we are processing over.\n",
    "proc_sids = hcpannot.subject_ids\n",
    "\n",
    "# The hemispheres we are processing over.\n",
    "proc_hemis = ['lh', 'rh']\n",
    "\n",
    "# The number of processes we want to use (None for all CPUs).\n",
    "nproc = 24\n",
    "\n",
    "# How we log information about the processing; typically just the print\n",
    "# function. Note that we always write out log-files containing the results of\n",
    "# processing each rater/sid/hemisphere; this is just about printing to the\n",
    "# notebook/screen.\n",
    "logfn = print\n",
    "\n",
    "# If we want to skip this step whenever the logfile already exists, we can set\n",
    "# the overwrite value to False. If this is True, then the export trace functions\n",
    "# will always be run.\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traces and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we process the traces.\n",
    "\n",
    "# Options we want to pass to the export_traces funciton.\n",
    "opts = {'overwrite': overwrite}\n",
    "# Make the job list.\n",
    "jobs = makejobs(proc_raters, proc_sids, proc_hemis, [trace_save_path], [opts])\n",
    "# Run this step in the processing.\n",
    "proc_traces_results = mpstep(export_traces, jobs, \"traces\",\n",
    "                             overwrite=overwrite,\n",
    "                             nproc=nproc,\n",
    "                             print=logfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we process the paths.\n",
    "\n",
    "# Options we want to pass to the export_paths funciton.\n",
    "opts = {'overwrite': overwrite}\n",
    "# Make the job list.\n",
    "jobs = makejobs(proc_raters, proc_sids, proc_hemis, [trace_save_path], [opts])\n",
    "# Run this step in the processing.\n",
    "proc_paths_results = mpstep(export_paths, jobs, \"paths\",\n",
    "                            overwrite=overwrite,\n",
    "                            nproc=nproc,\n",
    "                            print=logfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Contours, Traces, and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the mean contours.\n",
    "\n",
    "# Options we want to pass to the export_means funciton.\n",
    "opts = {'overwrite': overwrite}\n",
    "# Make the job list.\n",
    "jobs = makejobs(proc_sids, proc_hemis, [trace_save_path], [opts])\n",
    "# Run this step in the processing.\n",
    "proc_means_results = mpstep(export_means, jobs, \"means\",\n",
    "                            overwrite=overwrite,\n",
    "                            nproc=nproc,\n",
    "                            print=logfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we create the traces for the mean rater.\n",
    "\n",
    "# Options we want to pass to the export_traces funciton.\n",
    "opts = {'vc_contours': hcpannot.analysis.vc_contours_meanrater,\n",
    "        'overwrite': overwrite}\n",
    "# Make the job list.\n",
    "jobs = makejobs([meanrater], proc_sids, proc_hemis, [trace_save_path], [opts])\n",
    "# Run this step in the processing.\n",
    "proc_meantraces_results = mpstep(export_traces, jobs, \"meantraces\",\n",
    "                                 overwrite=overwrite,\n",
    "                                 nproc=nproc,\n",
    "                                 print=logfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create the paths for the mean rater.\n",
    "\n",
    "# Options we want to pass to the export_paths funciton.\n",
    "opts = {'overwrite': overwrite}\n",
    "# Make the job list.\n",
    "jobs = makejobs([meanrater], proc_sids, proc_hemis, [trace_save_path], [opts])\n",
    "# Run this step in the processing.\n",
    "proc_meanpaths_results = mpstep(export_paths, jobs, \"meanpaths\",\n",
    "                                overwrite=overwrite,\n",
    "                                nproc=nproc,\n",
    "                                print=logfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surface Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cell below will generate a large number of warnings about poor performance right toward the end of the analysis. This is because the mean subjects all have large numbers of points in their contours, and this causes poorer performance. Fortunately, these still run in a reasonable time, so the warnings can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating surface areas, we use okay and fail functions that return an\n",
    "# okaystr or failstr bur that also records a dict of data for the dataframe.\n",
    "def sarea_fail(err, tup):\n",
    "    msg = failstr(err, tup)\n",
    "    (rater, sid, h, save_path, opts) = tup\n",
    "    res = dict(\n",
    "        rater=rater, sid=sid, hemisphere=h,\n",
    "        hV4=np.nan, VO1=np.nan, VO2=np.nan, cortex=np.nan,\n",
    "        message=msg)\n",
    "    return res\n",
    "def sarea_okay(res, tup):\n",
    "    msg = okaystr(res, tup)\n",
    "    res = dict(res, message=msg)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We save the sarea file to this location.\n",
    "sarea_filename = os.path.join(trace_save_path, 'surface_areas.csv')\n",
    "# We use the standard raters plus the meanrater here.\n",
    "proc_sarea_raters = proc_raters + [meanrater]\n",
    "# Options we want to pass to calc_surface_areas.\n",
    "opts = {}\n",
    "# Make the job list.\n",
    "jobs = makejobs(proc_sarea_raters, proc_sids, proc_hemis,\n",
    "                [trace_save_path], [opts])\n",
    "# Multiprocess all the jobs.\n",
    "if overwrite or not os.path.isfile(sarea_filename):\n",
    "    proc_sarea_results = mprun(calc_surface_areas, jobs,\n",
    "                               nproc=nproc,\n",
    "                               print=logfn,\n",
    "                               onokay=sarea_okay,\n",
    "                               onfail=sarea_fail)\n",
    "    # Convert the surface area data to a dataframe and save it.\n",
    "    sarea_data = ny.to_dataframe(proc_sarea_results)\n",
    "    ny.save(sarea_filename, sarea_data)\n",
    "else:\n",
    "    sarea_data = ny.load(sarea_filename)\n",
    "    proc_sarea_results = None\n",
    "proc_sarea_results = [r['message'] for (ii,r) in sarea_data.iterrows()]\n",
    "sarea_data = sarea_data[['rater','sid','hemisphere','hV4','VO1','VO2','cortex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_nans = sarea_data[np.isnan(sarea_data['hV4'])]\n",
    "# Dataframe of raters and how many NaN surface areas they have.\n",
    "# The total number of entries should always be 362.\n",
    "raters = sa_nans['rater'].values\n",
    "missing_sarea_data = ny.to_dataframe(\n",
    "    [dict(rater=rater,\n",
    "          nancount=np.sum(sa_nans['rater'] == rater),\n",
    "          entrycount=np.sum(sarea_data['rater'] == rater))\n",
    "     for rater in np.unique(raters)])\n",
    "missing_sarea_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp = ny.data['hcp_lines'].retinotopy_sibling_pairs\n",
    "rsp = {\n",
    "    k: v[np.all(np.isin(v, subject_list_1), axis=1)]\n",
    "    for (k,v) in rsp.items()}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    mean_twin_sids = {k: v[np.random.choice(np.arange(v.shape[0]))]\n",
    "                      for (k,v) in rsp.items()\n",
    "                      if k != 'nontwin_siblings'}\n",
    "    mean_twin_trs = {}\n",
    "    for h in ['lh','rh']:\n",
    "        trs = {}\n",
    "        try:\n",
    "            for (k,(t1,t2)) in mean_twin_sids.items():\n",
    "                trs[k] = {t1:mean_traces(t1, h), t2:mean_traces(t2, h)}\n",
    "            mean_twin_trs[h] = trs\n",
    "        except Exception:\n",
    "            raise\n",
    "    if len(mean_twin_trs) == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig,axs) = plt.subplots(2,4, figsize=(4,2), dpi=72*8)\n",
    "fig.subplots_adjust(0,0,1,1,0.1,0.1)\n",
    "\n",
    "mask = ('prf_variance_explained', 0.025, 1)\n",
    "lims = ((-60,60), (-80,40))\n",
    "\n",
    "for ((h,trs),axrow) in zip(mean_twin_trs.items(), axs):\n",
    "    mz = trs['monozygotic_twins']\n",
    "    dz = trs['dizygotic_twins']\n",
    "    mm = {k:v for d in (mz,dz) for (k,v) in d.items()}\n",
    "    for (ax,(sid,trs)) in zip(axrow, mm.items()):\n",
    "        # Get this sujbect's flatmap\n",
    "        sub = ny.data['hcp_lines'].subjects[sid]\n",
    "        hem = sub.hemis[h]\n",
    "        fmap = hcpannot.op_flatmap(hem)\n",
    "        # Plot the flatmap.\n",
    "        ny.cortex_plot(fmap, color='prf_polar_angle', axes=ax,\n",
    "                       mask=mask)\n",
    "        sdat = hcpannot.interface.subject_data[(sid,h)]\n",
    "        for (k,v) in sdat['v123'].items():\n",
    "            (x,y) = v\n",
    "            ax.plot(x, y, 'w-', lw=0.25)\n",
    "        for (k,v) in trs.items():\n",
    "            (x,y) = v.points\n",
    "            ax.plot(x, y, 'k-', lw=0.5)\n",
    "        ax.axis('off')\n",
    "        ax.set_xlim(lims[0])\n",
    "        ax.set_ylim(lims[1])\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = {k: np.concatenate([v, v, v, v])\n",
    "           for k in ['rater','sid','hemisphere']\n",
    "           for v in [df[k].values]}\n",
    "n = len(df)\n",
    "df_long['visual_area'] = np.concatenate(\n",
    "    [[k]*n for k in ['hV4','VO1','VO2','cortex']])\n",
    "df_long['surface_area'] = np.concatenate(\n",
    "    [df[k].values for k in ['hV4','VO1','VO2','cortex']])\n",
    "df_long = ny.to_dataframe(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_plot = df_long[df_long['visual_area'] != 'cortex']\n",
    "ax = sns.violinplot(x=\"visual_area\", y=\"surface_area\", hue=\"hemisphere\",\n",
    "                    col='rater', split=True, data=df_plot, cut=0, lw=0.5)\n",
    "ax.set_ylabel(r'Surface Area [mm$^2$]')\n",
    "ax.set_xlabel('Visual Area')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_long[df_long['visual_area'] != 'cortex']\n",
    "sns.catplot(x=\"rater\", y=\"surface_area\", hue=\"hemisphere\",\n",
    "            col='visual_area', split=True, data=df_plot, cut=0,\n",
    "            kind=\"violin\")\n",
    "#ax.set_ylabel(r'Surface Area [mm$^2$]')\n",
    "#ax.set_xlabel('Visual Area')\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}