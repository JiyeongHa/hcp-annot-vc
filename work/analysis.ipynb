{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fe3bdc",
   "metadata": {},
   "source": [
    "# Analysis of Contours of the HCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f2ef9",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bc1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# The cache directories:\n",
    "cache_path       = Path('/data/crcns2021/hcpannot-cache')\n",
    "image_cache_path = cache_path / 'annot-images'\n",
    "v123_cache_path  = cache_path / 'annot-v123'\n",
    "csulc_cache_path = cache_path / 'annot-csulc'\n",
    "# The save path of the data branch:\n",
    "data_path = Path('/data/crcns2021/results/data_branch/save')\n",
    "# The processing path and subpaths:\n",
    "proc_path    = Path('/data/crcns2021/results/proc')\n",
    "traces_path  = proc_path / 'traces'\n",
    "paths_path   = proc_path / 'paths'\n",
    "means_path   = proc_path / 'means'\n",
    "labels_path  = proc_path / 'labels'\n",
    "reports_path = proc_path / 'reports'\n",
    "\n",
    "# The file of visual surface areas for the data.\n",
    "ventral_sarea_path = proc_path / 'ventral_sareas.tsv'\n",
    "dorsal_sarea_path = proc_path / 'dorsal_sareas.tsv'\n",
    "\n",
    "# The hcpannot library path; if hcpannot is not on the path for\n",
    "# this notebook, the notebook will try to figure out where it is\n",
    "# and will use this directory as a backup.\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# If you aren't using /data\n",
    "import os\n",
    "os.environ['HCPANNOT_LOAD_PATH'] = os.fspath(cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635d3b5",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3890de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pimms, pandas, json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipyvolume as ipv\n",
    "import torch\n",
    "import neuropythy as ny\n",
    "\n",
    "import hcpannot\n",
    "\n",
    "# Note the cache path we want to use outside the docker container;\n",
    "# normally this gets set by the Docker startup, so here outside\n",
    "# the docker container we set it manually.\n",
    "hcpannot.interface.default_load_path = str(cache_path)\n",
    "\n",
    "# The subject IDs we are processing over, as a numpy array.\n",
    "sids = np.array(hcpannot.config.subject_list)\n",
    "\n",
    "# The list of visual areas by region:\n",
    "from hcpannot.config import region_areas\n",
    "\n",
    "# A utility function for getting data from proc dictionaries.\n",
    "from hcpannot import nestget\n",
    "\n",
    "# The mean rater's name ('mean') and some plotting functions are also defined in\n",
    "# the hcpannot.analysis subpackage.\n",
    "from hcpannot.io import plot_contours\n",
    "\n",
    "# Finally, the proc and meanproc functions, which give us the processed data.\n",
    "from hcpannot.proc import proc, proc_meanrater, proc_meansub\n",
    "\n",
    "# Meta-data about the annotations.\n",
    "from hcpannot.config import ventral_raters, dorsal_raters, raters_by_region\n",
    "\n",
    "# The hemispheres we are processing over.\n",
    "hemis = ['lh', 'rh']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc403bac",
   "metadata": {},
   "source": [
    "### How to Examine the Processed Contour Data\n",
    "\n",
    "The `proc` function can be used to query processed data. `proc` returns a (lazy) dictionary of the processed data. If the requested data have not been previously calculated and cached, the `proc` function will do so, saving cached results to (and loading them from) the directory provided via the `save_path` option. The `load_path` refers to the directory from which the contours should be loaded, i.e. the `save` directory of the [`data` branch of the `noahbenson/hcp-annot-vc` GitHub repository](https://github.com/noahbenson/hcp-annot-vc/tree/data).\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "* Load (or prepare to calculate) the processed ventral contour data for rater `R1`, HCP subject `100610`, and the left hemisphere.  \n",
    "  ```python\n",
    "  >>> from hcpannot.proc import proc \n",
    "  >>> data = proc(\n",
    "  ...     'ventral',\n",
    "  ...     rater='R1',\n",
    "  ...     sid=100610,\n",
    "  ...     hemisphere='lh',\n",
    "  ...     save_path=proc_path,\n",
    "  ...     load_path=data_path)\n",
    "  ```\n",
    "* View the keys of the processed data dictionary.  \n",
    "  ```python\n",
    "  >>> sorted(data.keys())\n",
    "  ['chirality',\n",
    "   'contours',\n",
    "   'cortex',\n",
    "   'flatmap',\n",
    "   'hemisphere',\n",
    "   'io_options',\n",
    "   'label_weights',\n",
    "   'labelkey',\n",
    "   'labels',\n",
    "   'nested_data',\n",
    "   'paths',\n",
    "   'rater',\n",
    "   'region',\n",
    "   'reports',\n",
    "   'save_path',\n",
    "   'sid',\n",
    "   'traces']\n",
    "  ```\n",
    "* View the contours. Each contour is a `2xN` matrix of the (`x`,`y`)-values of the points clicked by the rater. The coordinates of the points are registered to the flatmap on which they were drawn, which is given by `data['flatmap']`.  \n",
    "  ```python\n",
    "  >>> data['contours']\n",
    "  {'hV4_VO1': array([[ 38.02148285,  32.13910222, ...],\n",
    "                     [-44.74639194, -46.70718548, ...]]),\n",
    "   'VO1_VO2': array([[ 39.00187962,  38.02148285, ...],\n",
    "                     [-44.25525238, -47.68664107, ...]]),\n",
    "   'hV4_outer': array([[-18.51013327, -19.33172823, ...],\n",
    "                       [-14.4353661 , -18.76493634, ...]]),\n",
    "   'VO_outer': array([[ 38.80127474,  43.90386347, ...],\n",
    "                      [-43.03350935, -44.74639194, ...]])}\n",
    "  ```\n",
    "* Plot a contour on the flatmap.  \n",
    "  ```python\n",
    "  >>> import neuropythy as ny, matplotlib.pyplot as plt\n",
    "  >>> color = 'prf_polar_angle'\n",
    "  >>> mask = ('prf_variance_explained', 0.1, 1)\n",
    "  >>> (fig,ax) = plt.subplots(1,1, figsize=(8,8))\n",
    "  >>> ny.cortex_plot(data['flatmap'], color=color, mask=mask, axes=ax)\n",
    "  >>> (x, y) = data['contours']['hV4_outer']\n",
    "  >>> ax.plot(x, y, 'wo-')\n",
    "  >>> plt.show()\n",
    "  ```\n",
    "\n",
    "Here's an example code-block using the `plot_contours` function to plot a set of boundaries along with the V1-V3 contours.\n",
    "\n",
    "```python\n",
    ">>> data = proc(\n",
    "...     'ventral',\n",
    "...     rater='R1',\n",
    "...     sid=100610,\n",
    "...     hemisphere='lh',\n",
    "...     save_path=proc_path,\n",
    "...     load_path=data_path)\n",
    "\n",
    "# Make the flatmap plot and the boundaries.\n",
    ">>> fig = plot_contours(dat, boundaries=True)\n",
    "\n",
    "# Extract the pyplot axes that were used.\n",
    ">>> ax = fig.axes[0]\n",
    "\n",
    "# Grab the subject data, which includes the V1-V3 contours.\n",
    ">>> sdat = hcpannot.interface.subject_data[(dat['sid'],dat['hemisphere'])]\n",
    "\n",
    "# And plot all of these contours:\n",
    ">>> for (x,y) in sdat['v123'].values():\n",
    "...     ax.plot(x, y, 'w-', lw=0.25)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Jupyter Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
